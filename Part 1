# Log transform to long tailed distributions
log_column_name = np.log(column_name)
# Polynomial transforms to non-linear distributions
https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html
# Calculated Fields
df['new_column'] = df['column1']/df['column2']
# Binning of variables - clustering
pd.cut(df['column_name'], n) - n equals the number of bins
# Correlations between variables - remove a variable/column
df.corr() - this is correlation matrix for the entire dataframe
np.corrcoef(x,y) - this is the correlation between x and y
# Add interactions for 2 variables
from sklearn.linear_model import LinearRegression
from sklearn.cross_validation import cross_val_score
from sklearn.cross_validation import KFold
regression = LinearRegression(normalize=True)
crossvalidation = KFold(n=X.shape[0], n_folds=10, shuffle=True, random_state=1)
df = pd.DataFrame(X,columns=boston.feature_names)
baseline = np.mean(cross_val_score(regression, df, y, scoring=‘r2’, cv=crossvalidation,
 n_jobs=1))
interactions = list()
for feature_A in boston.feature_names:
 for feature_B in boston.feature_names:
  if feature_A > feature_B:
   df[‘interaction’] = df[feature_A] * df[feature_B]
   score = np.mean(cross_val_score(regression, df, y, scoring=‘r2’,
    cv=crossvalidation, n_jobs=1))
   if score > baseline:
    interactions.append((feature_A, feature_B, round(score,3)))
print ‘Baseline R2: %.3f’ % baseline
print ‘Top 10 interactions: %s’ % sorted(interactions, key=lambda(x):x[2],
 reverse=True)[:10]
Baseline R2: 0.699
Top 10 interactions: [(‘RM’, ‘LSTAT’, 0.782), (‘TAX’, ‘RM’, 0.766),
 (‘RM’, ‘RAD’, 0.759), (‘RM’, ‘PTRATIO’, 0.75), 
(‘RM’, ‘INDUS’, 0.748), (‘RM’, ‘NOX’, 0.733), 
(‘RM’, ‘B’, 0.731), (‘RM’, ‘AGE’, 0.727), 
(‘RM’, ‘DIS’, 0.722), (‘ZN’, ‘RM’, 0.716)]

# Calculate variable significance
# - univariate relationship with dependent variable
# - correlation with dependent variable (discernable pattern?)
# - p value

# Feature Importance
from sklearn import datasets
from sklearn import metrics
from sklearn.ensemble import ExtraTreesClassifier
# load the iris datasets
dataset = datasets.load_iris()
# fit an Extra Trees model to the data
model = ExtraTreesClassifier()
model.fit(dataset.data, dataset.target)
# display the relative importance of each attribute
print(model.feature_importances_)

# standardized variables
from sklearn.preprocessing import scale
stand_column_name = scale(df[‘column_name’])
# change in Rsquared when variable is included/excluded

# one hot encoding a categorical variable using pandas get_dummies
features = pd.get_dummies(features)
